# -*- coding: utf-8 -*-
"""LAB-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y1ZWZRgaSlyeEbZqGXn8adNRzPwqjgE8
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Question-1
Write a Python code to build a deep neural network using Keras and compute a number of parameters, memory and FLOPs for the following model. Use relu activations functions in the hidden layers and sigmoid activations function in the output layers.


CPU that performs 1 GFLOPS (1,000,000,000) per seconds and computes the inference time of the Deep neural network model.



<img src="2.png" width="700" height="500">

##  Parameters calculation in Deep neural network

The number of internal parameters in a neural network is the total number of weights + the total number of biases. The total number of weights equals the sum of the products of each pair of adjacent layers. The total number of biases equals the number of hidden neurons + the number of output neurons.

## Model Size calculations


Model Size (in bytes)=Number of Parameters×Bytes Per Parameter
Model Size (in KB)=Model Size (in bytes)/1024

##  FLOPs calculation in Deep neural network
FLOPs of  FC=2*(input size x output size )+(output size x activation)

## Activation functions FLOPS for  Tensor Flow

Relu  -->      1FLOPs

Sigmoid   -->   1FLOPs

Tanh   -->      1FLOPs

Softmax    -->  6FLOPs

## Infrences time calculations

The inference time = FLOPs/FLOPS.

FLOPs-> measures computational complexity of the model.

FLOPS-> measures the hardware’s processing capability
"""

### Write  your code here
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

model=Sequential((Dense(5,activation="relu",input_shape=(5,)),Dense(5,activation="relu"),Dense(1,activation="relu"),))

def compute(model):
  totparm=0
  totflops=0
  for layer in model.layers:
    layertype=layer.__class__.__name__
    if layertype in ["Dense"]:
      inputunit=layer.input.shape[-1] # Accessing input shape using layer.input
      outputunit=layer.output.shape[-1]# Accessing input shape using layer.output
      parm=inputunit*outputunit+outputunit
      flops=2*inputunit*outputunit
      totparm=totparm+parm
      totflops=totflops+flops
  return totparm,totflops

p,f=compute(model)
print(p)
print(f)

"""# Question-2
Write a Python code to build a deep neural network using Keras and compute a number of parameters, memory and FLOPs for the following model. Use relu activations functions in the hidden layers and softmax activations function in the output layers. Write a Python code plot  the bar graph of the question 1 and question 2 output and compare.



<img src="1.png" width="700" height="500">
"""

### Write  your code here
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense

model=Sequential((Dense(9,activation="relu",input_shape=(4,)),Dense(6,activation="relu"),Dense(3,activation="relu"),))

def compute(model2):
  totparm=0
  totflops=0
  for layer in model.layers:
    layertype=layer.__class__.__name__
    if layertype in ["Dense"]:
      inputunit=layer.input.shape[-1] # Accessing input shape using layer.input
      outputunit=layer.output.shape[-1]# Accessing input shape using layer.output
      parm=inputunit*outputunit+outputunit
      flops=2*inputunit*outputunit
      totparm=totparm+parm
      totflops=totflops+flops
  return totparm,totflops

p,f=compute(model)
print(p)
print(f)

"""## Output Dimensions Formula for 2D Convolution
<img src="10.png" width="600" height="400">

## Parameter calculation of 2DCNN


<img src="4.png" width="400" height="200">

## FLOPs calculation of 2DCNN
<img src="6.png" width="600" height="400">



## FLOPs calculation for Pooling Layers
<img src="12.png" width="600" height="400">

# Question-3

Write a Python code build 2DCNN model for the following specifications using Keras and compute the number of parameters ,model size and FLOPs

The model architecture consists of several layers designed for image classification tasks, such as recognizing digits from the MNIST dataset. The architecture begins with a 2D convolutional layer (Conv2D), which applies 32 filters of size 3x3 to the input image (28x28x1), followed by the ReLU activation function to introduce non-linearity. This is followed by a max-pooling layer (MaxPooling2D) with a pool size of 2x2, reducing the spatial dimensions of the feature maps while retaining important information. A second convolutional layer with 64 filters of size 3x3 is then applied, again using ReLU activation. Another max-pooling layer  (2x2 ) follows to further downsample the feature maps. The output of the convolutional layers is then flattened into a one-dimensional vector using the Flatten layer, which is fed into the fully connected dense layers. The first dense layer has 64 neurons with ReLU activation, allowing the model to learn complex representations, while the final dense layer has 10 neurons with a softmax activation function, providing probabilities for each of the 10 possible digit classes.
"""



import tensorflow as tf
from tensorflow.keras import layers, models

# Define the model architecture
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Function to compute parameters and FLOPs
def compute(model):
    totparm = 0
    totflops = 0

    # Build the model to get output shapes
    model.build(input_shape=(None, 28, 28, 1))

    for layer in model.layers:
        layertype = layer.__class__.__name__

        if layertype == "Dense":
            inputunit = layer.input.shape[-1]  # Accessing input shape using layer.input
            outputunit = layer.output.shape[-1]  # Accessing output shape using layer.output
            parm = inputunit * outputunit + outputunit  # Parameters: (input_units * output_units) + output_units (bias)
            flops = 2 * inputunit * outputunit  # FLOPs for Dense: input_units * output_units * 2
            totparm += parm
            totflops += flops

        elif layertype == "Conv2D":
            # Get the input channels, output channels, and kernel size
            input_channels = layer.input.shape[-1]  # Number of input channels (depth of input)
            output_channels = layer.filters  # Number of filters (output channels)
            kernel_size = layer.kernel_size[0] * layer.kernel_size[1]  # Kernel size (height * width)

            # Compute output shape of Conv2D layer manually
            input_height, input_width = layer.input.shape[1], layer.input.shape[2]  # Input height and width
            output_height = (input_height - layer.kernel_size[0]) // layer.strides[0] + 1
            output_width = (input_width - layer.kernel_size[1]) // layer.strides[1] + 1

            # FLOPs for Conv2D: kernel_height * kernel_width * input_channels * output_channels * output_height * output_width * 2 (multiplications + additions)
            flops = 2 * kernel_size * input_channels * output_channels * output_height * output_width
            totflops += flops

            # Parameters for Conv2D: (kernel_height * kernel_width * input_channels * output_channels) + output_channels (bias)
            parm = kernel_size * input_channels * output_channels + output_channels
            totparm += parm

    return totparm, totflops

# Compute the total parameters and FLOPs for the model
total_params, total_flops = compute(model)

# Print the results
print(f"Total parameters: {total_params}")  # Expected: 121930
print(f"Total FLOPs: {total_flops}")  # Expected: 5106520

"""# Question-4
Write a Python code to build CNN using Keras and compute a  number of parameters,memory and FLOPs for the following model.





<img src="3.jpg" width="900" height="700">
"""

### Write  your code here
### Write  your code here

model4=Sequential([
    Conv2D(filters=32, kernel_size=(3, 3), activation="relu", input_shape=(32, 32, 3)),
    MaxPooling2D(pool_size=(2,2)),

    Conv2D(filters=64,kernel_size=(3,3),activation="relu"),
    MaxPooling2D(pool_size=(2,2)),

    Flatten(),
    Dense(64,activation="relu"),
    Dense(10,activation="softmax")
])
model4.summary()

"""## Output Shape of 3DCNN

<img src="13.png" width="400" height="300">
<img src="14.png" width="400" height="300">






### 3DCNN parameters calculations
<img src="7.png" width="900" height="700">

### 3DCNN FLOPs calculations

<img src="8.png" width="900" height="700">



### 3DCNN FLOPs calculation for Pooling Layers
<img src="15.png" width="900" height="700">

# Question-5

You are tasked with designing a 3D Convolutional Neural Network (3D CNN) to classify video clips into one of five categories, such as walking, running, jumping, swimming, and cycling. Each video clip consists of 16 frames of size 64x64, and the data has a single channel (grayscale). The model should include two 3D convolutional layers followed by max-pooling layers, a flattening layer, and fully connected dense layers. Specifically, the architecture should satisfy the following requirements:

The input layer should accept a shape of (16, 64, 64, 1) corresponding to the temporal, height, width, and channel dimensions.
The first 3D convolutional layer should have 32 filters of size (3, 3, 3) and use ReLU activation.
The first max-pooling layer should have a pool size of (2, 2, 2) to downsample the feature maps.
The second 3D convolutional layer should have 64 filters of size (3, 3, 3) and use ReLU activation.
The second max-pooling layer should again have a pool size of (2, 2, 2).
The flattened layer should connect to a dense layer with 128 neurons using ReLU activation, followed by the output layer with 5 neurons and a softmax activation.
Design and implement this 3D CNN architecture, compute the number of parameters for each layer, compute the model size and compute the FLOPs.
"""

### Write  your code here
import tensorflow as tf
from tensorflow.keras import layers, models

# Build the model
model = models.Sequential([
    layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=(16, 64, 64, 1)),
    layers.MaxPooling3D((2, 2, 2)),
    layers.Conv3D(64, (3, 3, 3), activation='relu'),
    layers.MaxPooling3D((2, 2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(5, activation='softmax')
])

# Summary of the model to get parameters
model.summary()

"""# Question-6

A company is building a system to predict customer sentiment (positive or negative) based on a sequence of customer reviews.
Each review is represented as a feature vector of size 4, where each feature corresponds to a specific aspect of the review, such as tone, length, and keyword presence. To process this sequential data, the team decides to use a Recurrent Neural Network (RNN).

The input size n<sub>x</sub> is 4, meaning each input vector x<sup>t</sup> has 4 features.  
The hidden layer has 3 hidden units n<sub>a</sub>=3.  
The output size n<sub>y</sub> is 2, corresponding to the two possible sentiment classes (positive or negative).  
The sequence length T<sub>x</sub> is 5, meaning the RNN will process a sequence of 5 reviews at a time.
use sigmoid activation functions in the output layers and compute the number of parameters and memory in the RNN model.




<img src="9.png" width="300" height="100">


Number of parameter of RNN = g × [a(a+i) + a]

a --> hidden unit

i ---> input unit


"""

### Write  your code here
import tensorflow as tf
from tensorflow.keras import layers, models

# Define the model
model = models.Sequential([
    # RNN layer with 3 hidden units and 4 input features (nx = 4)
    layers.SimpleRNN(3, input_shape=(5, 4), activation='tanh', return_sequences=False),
    # Output layer with 2 neurons (for binary sentiment classification)
    layers.Dense(2, activation='sigmoid')
])

# Print the model summary to check the number of parameters
model.summary()

# Calculate and print the number of parameters and memory
def compute_parameters_and_memory(model):
    # Total parameters in the model
    total_params = model.count_params()

    # Memory usage (in bytes) for the parameters (assuming 4 bytes per parameter)
    memory_usage = total_params * 4  # 4 bytes per parameter (float32)

    return total_params, memory_usage

# Get the number of parameters and memory usage
total_params, memory_usage = compute_parameters_and_memory(model)

# Display the results
print(f"Total number of parameters: {total_params}")
print(f"Memory required for the parameters: {memory_usage} bytes")

"""# Question 7

Write a Python code to implement a single LSTM unit for the follwoing and compute the parameter of the follwoing model using Keras.
    
<img src="https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_internal2.png?raw=true" width="500">


 Notice that we can guess the size (shape) of W,U and b given:
 * Input size ($h_{t-1}$ and $x_{t}$ )
 * Output size ($h_{t-1}$)

 Since output must equal to Hidden State (hx1) size:

  * for W param =  ($h$ × $x$)
  * for U param =  ($h$ × $h$)
  * for Biases  param =   $h$

 * total params = W param + U param + Biases param
  
    =  ($h$ × $x$) +  ($h$ × $h$) +  $h$

    =  ( ($h$ × $x$) +  ($h$ × $h$) +   $h$ )

    =  ( ($x$ + $h$) ×  $h$  +   $h$ )

* there are 4 functions which are exactly defined in the same way, in the LSTM layer, there will be

 ##   **LSTM parameter number = 4 × (($x$ + $h$) × $h$ +$h$)**


"""

### Write  your code here
import tensorflow as tf
from tensorflow.keras import layers, models

# Define the LSTM model with a single LSTM unit
input_size = 4  # Number of features in the input vector (x)
hidden_units = 3  # Number of hidden units in the LSTM layer (h)

# Build the model with one LSTM unit
model = models.Sequential([
    layers.LSTM(hidden_units, input_shape=(None, input_size), return_sequences=False)
])

# Print model summary to see the number of parameters
model.summary()

# Function to calculate number of parameters for a single LSTM unit
def compute_lstm_parameters(input_size, hidden_units):
    # Number of parameters in a single LSTM unit
    params_per_gate = (input_size + hidden_units) * hidden_units + hidden_units
    total_params = 4 * params_per_gate  # Four gates (input, forget, cell, output)

    return total_params

# Compute the number of parameters
total_lstm_params = compute_lstm_parameters(input_size, hidden_units)

# Print the calculated total parameters
print(f"Total parameters in the LSTM unit: {total_lstm_params}")